{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "    <img style=\"float: right; padding-right: 10px; width: 65px\" src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/clemson_paw.png\"> </div>\n",
    "\n",
    "\n",
    "## Week 6: Clustering\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Fall 2021**<br>\n",
    "**Instructor(s):** Nina Hubig <br>\n",
    "**Author(s):** Brandon Walker, Chris Kalahiki\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS CELL TO GET THE RIGHT FORMATTING \"\"\"\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "css_file = 'https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/css/cpsc6300.css'\n",
    "styles = requests.get(css_file).text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"exercise\"><b>Question:</b> Why do we care about clustering? How/why is it useful?</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Clustering Algorithms\n",
    "\n",
    "We will now walk through three clustering algorithms, first discussing them at a high-level, then showing how to implement them with Python libraries. Let's first load and scale our data, so that particular dimensions don't naturally dominate in their contributions in the distant calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads and displays our summary statistics of our data\n",
    "multishapes = pd.read_csv(\"multishapes.csv\")\n",
    "multishapes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_df = multishapes[['x','y']]\n",
    "ms_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scales our data\n",
    "scaled_df = pd.DataFrame(preprocessing.scale(ms_df), \n",
    "                         index=multishapes['shape'], \n",
    "                         columns=ms_df.columns)\n",
    "scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots our data\n",
    "msplot = scaled_df.plot.scatter(x='x',y='y',c='Black',title=\"Multishapes data\",figsize=(11,8.5))\n",
    "msplot.set_xlabel(\"X\")\n",
    "msplot.set_ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means clustering:\n",
    "\n",
    "### Code (via `sklearn`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise #1</b>: </div>\n",
    "\n",
    "For the first exercise:\n",
    "- Create a KMeans object using 3 clusters and a random_state of 109.\n",
    "- Fit the KMeans object to the scaled_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise1-solution.py\n",
    "ms_kmeans = KMeans(n_clusters = 3, random_state = 109).fit(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Just 1 line of code!\n",
    "\n",
    "Now that we've run k-Means, we can look at various attributes of our clusters. Full documenation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ms_kmeans.cluster_centers_)\n",
    "display(ms_kmeans.labels_[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "Take note of matplotlib's `c=` argument to color items in the plot, along with our stacking two different plotting functions in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(scaled_df['x'], scaled_df['y'], c=ms_kmeans.labels_);\n",
    "plt.scatter(ms_kmeans.cluster_centers_[:,0],\n",
    "            ms_kmeans.cluster_centers_[:,1], \n",
    "            c='r', marker='h', s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Question</b>: Is this expected or did something go wrong? Should we always scale our data before clustering?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of Clusters: Inertia\n",
    "Inertia measures the total squared distance from points to their cluster's centroid. We obviously want this distance to be relatively small. If we increase the number of clusters, it will naturally make the average distance smaller. If every point has its own cluster, then our distance would be 0. That's obviously not an ideal way to cluster. One way to determine a reasonable number of clusters to simply try many different clusterings as we vary **k**, and each time, measure the overall inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wss = []\n",
    "for i in range(1,11):\n",
    "    fitx = KMeans(n_clusters=i, init='random', n_init=5, random_state=109).fit(scaled_df)\n",
    "    wss.append(fitx.inertia_)\n",
    "\n",
    "plt.figure(figsize=(11,8.5))\n",
    "plt.plot(range(1,11), wss, 'bx-')\n",
    "plt.xlabel('Number of clusters $k$')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method showing the optimal $k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the place(s) where distance stops decreasing as much (i.e., the 'elbow' of the curve). It seems that 4 would be a good number of clusters, as a higher *k* yields diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of Clusters: Silhouette\n",
    "\n",
    "Let's say we have a data point $i$, and the cluster it belongs to is referred to as $C(i)$. One way to measure the quality of a cluster $C(i)$ is to measure how close its data points are to each other (within-cluster) compared to nearby, other clusters $C(j)$. This is what `Silhouette Scores` provide for us. The range is [-1,1]; 0 indicates a point on the decision boundary (equal average closeness to points intra-cluster and out-of-cluster), and negative values mean that datum might be better in a different cluster.\n",
    "\n",
    "Specifically, let $a(i)$ denote the average distance data point $i$ is to the other points in the same cluster:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/silhouette_intra.png\">\n",
    "</center>\n",
    "\n",
    "Similarly, we can also compute the average distance that data point $i$ is to all **other** clusters. The cluster that yields the minimum distance is denoted by $b(i)$:  \n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/silhouette_outer.png\">\n",
    "</center>\n",
    "\n",
    "Hopefully our data point $i$ is much closer, on average, to points within its own cluster (i.e., $a(i)$ than it is to its closest neighboring cluster $b(i)$). The silhouette score quantifies this as $s(i)$:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/silhouette_eq.png\">\n",
    "</center>\n",
    "\n",
    "**NOTE:** If data point $i$ belongs to its own cluster (no other points), then the silhouette score is set to 0 (otherwise, $a(i)$ would be undefined).\n",
    "\n",
    "The silhouette score plotted below is the **overall average** across all points in our dataset.\n",
    "\n",
    "The `silhouette_score()` function is available in `sklearn`. We can manually loop over values of K (for applying k-Means algorithm), then plot its silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "scores = [0]\n",
    "for i in range(2,11):\n",
    "    fitx = KMeans(n_clusters=i, init='random', n_init=5, random_state=109).fit(scaled_df)\n",
    "    score = silhouette_score(scaled_df, fitx.labels_)\n",
    "    scores.append(score)\n",
    "    \n",
    "plt.figure(figsize=(11,8.5))\n",
    "plt.plot(range(1,11), np.array(scores), 'bx-')\n",
    "plt.xlabel('Number of clusters $k$')\n",
    "plt.ylabel('Average Silhouette')\n",
    "plt.title('The Elbow Method showing the optimal $k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing all Silhoutte scores for a particular clustering\n",
    "\n",
    "Below, we borrow from an `sklearn` example. The second plot may be overkill.\n",
    " - The second plot is just the scaled data.\n",
    " - If you only need the raw silhouette scores, use the `silhouette_samples()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "#modified code from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "def silplot(X, clusterer, pointlabels=None):\n",
    "    cluster_labels = clusterer.labels_\n",
    "    n_clusters = clusterer.n_clusters\n",
    "    \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(11,8.5)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    \n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters = \", n_clusters,\n",
    "          \", the average silhouette_score is \", silhouette_avg,\".\",sep=\"\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(0,n_clusters+1):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=200, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "    xs = X[:, 0]\n",
    "    ys = X[:, 1]\n",
    "    \n",
    "    if pointlabels is not None:\n",
    "        for i in range(len(xs)):\n",
    "            plt.text(xs[i],ys[i],pointlabels[i])\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % int(i), alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means with 3 clusters\n",
    "ms_kmeans = KMeans(n_clusters=3, init='random', n_init=3, random_state=109).fit(scaled_df)\n",
    "\n",
    "# plot a fancy silhouette plot\n",
    "silplot(scaled_df.values, ms_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise #2</b>: </div>\n",
    "\n",
    "Using the silhouette scores' optimal number of clusters (per the elbow plot above):\n",
    "- Fit a new k-Means model with that many clusters\n",
    "- Plot the clusters like we originally did with k-means\n",
    "- Plot the silhouette scores just like the above cells\n",
    "- Which seems like a better clustering (i.e., 3 clusters or the number returned by the elbow plot above)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "ms_kmeans = KMeans(n_clusters = 4, random_state = 109).fit(scaled_df)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(scaled_df['x'], scaled_df['y'], c=ms_kmeans.labels_);\n",
    "plt.scatter(ms_kmeans.cluster_centers_[:,0],\n",
    "            ms_kmeans.cluster_centers_[:,1], \n",
    "            c='r', marker='h', s=100);\n",
    "\n",
    "silplot(scaled_df.values, ms_kmeans)\n",
    "\n",
    "#The number returned by the elbow plot is a better cluster because it is accounting for the cluster on the bottom right that was not considered when the cluster was 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of Clusters: Gap Statistic\n",
    "The gap statistic compares within-cluster distances (like in silhouette), but instead of comparing against the second-best existing cluster for that point, it compares our clustering's overall average to the average we'd see if the data were generated at random (we'd expect randomly generated data to not necessarily have any inherit patterns that can be easily clustered).\n",
    "\n",
    "In essence, the within-cluster distances (in the elbow plot) will go down just becuse we have more clusters. We additionally calculate how much they'd go down on non-clustered data with the same spread as our data and subtract that trend out to produce the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to install the gap_statistic package\n",
    "# !pip install gap_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gap_statistic import OptimalK\n",
    "\n",
    "gs_obj = OptimalK()\n",
    "\n",
    "n_clusters = gs_obj(scaled_df.values, n_refs=50, cluster_array=np.arange(1, 15))\n",
    "print('Optimal clusters: ', n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_obj.gap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_obj.plot_results() # makes nice plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to add error bars to help us decide how many clusters to use, the following code displays such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gapstat_with_errbars(gap_df):\n",
    "    gaps = gap_df[\"gap_value\"].values\n",
    "    diffs = gap_df[\"diff\"]\n",
    "    \n",
    "    err_bars = np.zeros(len(gap_df))\n",
    "    err_bars[1:] = diffs[:-1] - gaps[:-1] + gaps[1:]\n",
    "\n",
    "    plt.scatter(gap_df[\"n_clusters\"], gap_df[\"gap_value\"])\n",
    "    plt.errorbar(gap_df[\"n_clusters\"], gap_df[\"gap_value\"], yerr=err_bars, capsize=6)\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Gap Statistic\")\n",
    "    plt.show()\n",
    "    \n",
    "display_gapstat_with_errbars(gs_obj.gap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the `gap_stat` package, please see [the full documentation here](https://github.com/milesgranger/gap_statistic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "### Code (via `scipy`):\n",
    "\n",
    "There are many different cluster-merging criteria, one of which is Ward's criteria. Ward's optimizes having the lowest total within-cluster distances, so it merges the two clusters that will harm this objective least.\n",
    "`scipy`'s agglomerative clustering function implements Ward's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as hac\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "plt.figure(figsize=(11,8.5))\n",
    "dist_mat = pdist(scaled_df, metric=\"euclidean\")\n",
    "ward_data = hac.ward(dist_mat)\n",
    "\n",
    "hac.dendrogram(ward_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "How do you read a plot like the above? What are valid options for number of clusters, and how can you tell? Are some more valid than others? Does it make sense to compute silhouette scores for an agglomerative clustering? If we wanted to compute silhouette scores, what would we need for this to be possible?</div>\n",
    "\n",
    "### Lessons:\n",
    " - It's expensive: O(n^3) time complexity and O(n^2) space complexity.\n",
    " - Many choices for linkage criteria\n",
    " - Every node gets clustered (no child left behind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBscan Clustering\n",
    "DBscan uses an intuitive notion of denseness to define clusters, rather than defining clusters by a central point as in k-means.\n",
    "\n",
    "### Code (via `sklearn`):\n",
    "DBscan is implemented in good 'ol sklearn, but there aren't great automated tools for searching for the optimal `epsilon` parameter. For full documentation, please [visit this page](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "plt.figure(figsize=(11,8.5))\n",
    "\n",
    "fitted_dbscan = DBSCAN(eps=0.2).fit(scaled_df)\n",
    "\n",
    "plt.scatter(scaled_df['x'], scaled_df['y'], c=fitted_dbscan.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the dark purple dots are not clustered with anything else. They are lone singletons. You can validate such by setting epsilon to a very small value, and increase the min_samples to a high value. Under these conditions, nothing would cluster, and yet all dots become dark purple.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"exercise\"><b>Practice</b>: Experiment with the above code by changing its epsilon value and the min_samples (what is the default value for it, since the above code doesn't specify a value?)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just empirically observing how the epsilon value affects the clustering (which would be very costly for large, high-dimensional data), we can also inspect how far each data point is to its $N^{th}$ closest neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# x-axis is each individual data point, numbered by an artificial index\n",
    "# y-axis is the distance to its 2nd closest neighbor\n",
    "def plot_epsilon(df, min_samples):\n",
    "    fitted_neigbors = NearestNeighbors(n_neighbors=min_samples).fit(df)\n",
    "    distances, indices = fitted_neigbors.kneighbors(df)\n",
    "    dist_to_nth_nearest_neighbor = distances[:,-1]\n",
    "    plt.plot(np.sort(dist_to_nth_nearest_neighbor))\n",
    "    plt.xlabel(\"Index\\n(sorted by increasing distances)\")\n",
    "    plt.ylabel(\"{}-NN Distance (epsilon)\".format(min_samples-1))\n",
    "    plt.tick_params(right=True, labelright=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epsilon(scaled_df, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lessons:\n",
    " - Can cluster non-linear relationships very well; potential for more natural, arbritrarily shaped groupings\n",
    " - Does not require specifying the # of clusters (i.e., **k**); the algorithm determines such\n",
    " - Robust to outliers\n",
    " - Very sensitive to the parameters (requires strong knowledge of the data)\n",
    " - Doesn't guarantee that every (or ANY) item will be clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d9f3b65298fc7e0901db121425be03a9139ffdca794b528695071bf6d85a8cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
